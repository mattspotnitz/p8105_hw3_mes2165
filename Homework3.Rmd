---
title: "Homework3"
output: github_document
---
#Initialization
````{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
I will load the libraries and the dataset.


```{r}
library(p8105.datasets)
library(tidyverse)
library(readxl)
library(haven)
data("instacart")
```

I will inspect the dataset.

```{r}
df = instacart
df = janitor::clean_names(df)
str(df)
head(df)
tail (df)
```

There are 1384167 observations and 15 variables. Of those variables, 4 are characters.  Some key variables are order identiifer, product identifier, customer identifier, order day of the week, order hour, aisle identiier and aisle name. From the header, we see one example of an obseration is that the product "Bulgarian Yogurt" was in asile # 120.

I will count the number of aislde IDs and their frequencies. I will sort the frequencies in descending order.
```{r}
df %>% group_by(aisle_id) %>% summarize(n_obs = n()) %>% arrange(desc(n_obs))
```
On the basis of this output, there are 134 aisle IDs. The aisle ids with the highest frequencies are 83, 24, 123, 120, 21, 115, 84, 107, 91, 112.

I will make a data frame that filters to aisles with frequencies > 10,000, and then plot frequencies (or number of items) vs. Aisle ID.
```{r}
df_filter = df %>% group_by(aisle_id) %>% summarize(n_obs = n()) %>% filter(n_obs > 10000) %>% arrange(aisle_id)

ggplot(df_filter, aes(x = aisle_id, y = n_obs)) + geom_point() + 
  labs( 
    title = "Number of Items vs. Aisle ID",
    x = "Aisle ID",
    y = "Number of Items",
    caption = "Data from Instacart"
  )  + theme_minimal()

pdf('mes2165_homework3_problem1.pdf') #This code exports the plot
```
Now I will make the table of the most common items it the aisles named "baking ingredients", "dog food care" or "packaged vegetables fruits".  I will filter on those aisles, group them, count them, and then filter to the 3 most common items.

```{r}
df_food = df %>% filter (aisle == "baking ingredients" | aisle ==  "dog food care" | aisle == "packaged vegetables fruits") %>% group_by(aisle) %>% count(product_id, product_name) #I am filtering on baking ingredients, dog food care, and packaged vegetables aisles, and counting the product frequencies.

df_food_filter = df_food %>% filter(rank(desc(n))<= 3) %>% arrange(aisle, desc(n)) #I am filtering on the 3 most common items.
                    
df_food_filter #This code helps me iew the dataset
```

Table of hour of the day when pink lady apples and coffee ice cream are ordered.
```{r}
mutate_df = df  %>% filter ((product_name == "Pink Lady Apples") | (product_name == "Coffee Ice Cream")) %>% group_by(product_name, order_dow) %>% summarize(mean_hour = mean(order_hour_of_day))  #I am filtering on pink laddy apples and coffe ice cream. Also, I am calculating the mean hourof day for the order.

#Now I will make a pivot table for days of the week
mutate_df %>%  pivot_wider(names_from = order_dow, values_from = mean_hour)
```

#Problem 2
I will load the dataset for problem 2, and inspect the head and sturcture of the data set.
```{r}
library(p8105.datasets)
data("brfss_smart2010")
df_two = brfss_smart2010
df_two = janitor::clean_names(df_two)

head(df_two)
str(df_two)
```
I will filter on the "Overall Health" and change the class of the response variable into factor, exclude entries that are not between poor and excellent, and order the factors from 1 = poor to 5 = excellent.
```{r}
df_two_filter = df_two %>% filter(topic == "Overall Health") %>% filter (response != c("Poor", "Fair", "Good", "Very good", "Excellent")) %>% mutate(response = factor(response, levels =c("Poor", "Fair", "Good", "Very good", "Excellent") )) 

head(df_two_filter)
```
I will identify the states that were observed in 7 or more locations in 2002.
```{r} 
df_2002 = df_two_filter %>% filter (year == 2002) %>% group_by (locationabbr) %>% count() %>% filter(n>= 7) %>% arrange(desc(n))

df_2002_states = df_2002 %>% pull(locationabbr)
df_2002_states
```
The states that were observed in 7 or more locations in 2002 were: PA, MA, NJ, CT, FL, NC, MD, NH, NY, UT, RI, CO, MI, MN, WA, OH, HI, VT, DE, GA, IL, LA, NE, OK, OR, SC, KS, AZ, ID, IN, ME, MO, NV, SD, TN, TX.

I will repeat this process to identify the states that were observed in 7 or more locations in 2010.
```{r} 
df_2010 = df_two_filter %>% filter (year == 2010) %>% group_by (locationabbr) %>% count() %>% filter(n>= 7) %>% arrange(desc(n))

df_2010_states = df_2010 %>% pull(locationabbr)
df_2010_states
```
The states that were observed in 7 or more locations in 2002 were: FL, NJ, TX, MD, CA, NC, NE, WA, MA, NY, OH, SC, CO, PA, VT, ID, ME, NM, UT, LA, CT, MN, NH, RI, TN, HI, GA, MI, OR, KS, AL, AR, AZ, DE, IN, MO, MT, ND, OK, NV, IA, IL, MS, SD, WY

I will make a data set that is limited to excellent responses and contains, year, state, and a variable that averages the data_value across locations within a state.
```{r}
df_two_excellent = df_two_filter %>% filter(response == "Excellent") %>% group_by (locationabbr, locationdesc, year) %>% summarize (mean_data_value = mean(data_value)) %>% rename(state = locationabbr) %>% rename(location = locationdesc) ##I renamed "locationabbr" and "locationddec" to more intuitive names.

view(df_two_excellent)
```
I will now make a spaghetti plot of of average data value over time within a state.
```{r}
ggplot(df_two_excellent, aes(x = year, y = mean_data_value, group = state)) + geom_line()+
  labs(
    title = "Spaghetti Plot of Average Data Value by Year and State",
    x = "Year",
    y = "Mean Data Value",
    caption = "Data from BRFSS"
  )  + theme_minimal()
pdf('mes2165_homework3_problem2_spaghetti.pdf')
```
I will make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

First I will filter the data and rename some columns.
```{r}
df_two_NY = df_two_filter %>% filter((year == 2006) | (year==2010)) %>% filter (response == c("Poor", "Fair", "Good", "Very good", "Excellent")) %>% filter (locationabbr == "NY") %>% rename(state = locationabbr) %>% rename(location = locationdesc)
head(df_two_NY)
tail(df_two_NY)
```
Now, I will plot the data.
```{r}
ggplot(df_two_NY, aes(x = response, y = data_value, group = location)) + geom_point() + facet_wrap(.~year) +
  labs(
    title = "Data Value Distribution for 2006 and 2010 by Response and NY Location",
    x = "Response",
    y = "Data Value",
    caption = "Data from BRFSS"
  )  + theme_minimal()
pdf('mes2165_homework3_problem2_Data Value Distributions NY.pdf')
```
#Problem 3
I will load and view the data set
```{r}
df_accel = read_csv("accel_data.csv")
df_accel = janitor::clean_names(df_accel)
head(df_accel)
tail(df_accel)
str(df_accel)
view(df_accel)
```
I will make a weekend or weekday and weekend variables, which will be integer values. Then, I will convert the days to a factor variable.
```{r}
df_accel_edit = df_accel %>% mutate(weekday = (day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"))) %>% mutate(weekend = (day %in% c("Saturday", "Sunday"))) %>%  mutate(weekend = as.integer(as.logical(weekend))) %>% mutate(weekday = as.integer(as.logical(weekday))) %>% mutate(day = as.factor(day))

#I am checking the accuracy of the weekday and weekend variables
df_accel_edit %>% pull(day, weekday)
df_accel_edit %>% pull(day, weekend)
```
I will now describe the resulting dataset.
```{r}
str(df_accel_edit)
view(df_accel_edit)
```
There are a total of 35 rows and 1445 columns.  Day is a character variable, week and day id are integer variables, and weekend and weekday are integer variables. The rest are double variables that record activity by the minute.

I will create a variable for activity by day. I will take the sum activity over all columns and view the result.
```{r}
df_accel_day = df_accel_edit %>% group_by(day_id) %>% summarize(sum_activity = sum(activity_1:activity_1440), day, weekday, weekend) %>% arrange(day_id) # I grouped by day, and summed over all activity columns and

view(df_accel_day)
knitr::kable(df_accel_day) #I tabulated the results
```
Overall, the patient shows low activity with the excpetion of some days of high activity. 

To look for trends, I will make scatter plot of activity by day_id
```{r}
library(patchwork)
ggplot(df_accel_day, aes(x=day_id, y= sum_activity, color = day)) + geom_point() +
    labs(
    title = "Daily Activity vs. Day ID",
    x = "Day ID",
    y = "Daily Activity (counts)",
    caption = "Accelerometer Data"
  )  + theme_minimal()
pdf('mes2165_homework3_problem3_trend.pdf')
```
There are some days where the patient was very active, otherwise the patient's activity was low. The patient was more active on 2 Mondays and 2 Thursdays, 1 Friday, 1 Sunday, and  Tuesday.
